# pytorch-adamaio
All-In-One Adam Optimizer where several novelties are combined from following papers:

1) Decoupled Weight Decay Regularization for Adam
https://arxiv.org/abs/1711.05101

2) Online Learning Rate Adaptation with Hypergradient Descent
https://arxiv.org/abs/1703.04782

3) Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks
https://arxiv.org/abs/1711.05101
